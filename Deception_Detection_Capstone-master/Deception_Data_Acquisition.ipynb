{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Web-Scraping\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "\n",
    "# Packages for Saving File after Scraping\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url 1st page info, response object, and sample\n",
    "url = 'https://www.politifact.com/truth-o-meter/statements/?page=1'\n",
    "response = get(url)\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st page BeautifulSoup object, confirm type\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "type(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding each row of data we want to scrape, confirm type, check length\n",
    "statement_containers = html_soup.find_all('div', class_ = 'scoretable__item')\n",
    "print(type(statement_containers))\n",
    "print(len(statement_containers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data\n",
    "s = statement_containers[1]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statement\n",
    "s.find('p', class_ = 'statement__text').get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statement source\n",
    "s.find('div', class_ = 'statement__source').get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statement link\n",
    "s.find('p', class_ = 'statement__text').a[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statement veracity\n",
    "s.img[\"alt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the scraped data in\n",
    "statement = []\n",
    "source = []\n",
    "link = []\n",
    "veracity = []\n",
    "\n",
    "# Extract data from individual container\n",
    "for container in statement_containers:\n",
    "# statement\n",
    "    sta = container.find('p', class_ = 'statement__text').get_text(strip=True)\n",
    "    statement.append(sta)\n",
    "# source\n",
    "    sou = container.find('div', class_ = 'statement__source').get_text(strip=True)\n",
    "    source.append(sou)\n",
    "# link\n",
    "    lin = container.find('p', class_ = 'statement__text').a[\"href\"]\n",
    "    link.append(lin)\n",
    "# veracity\n",
    "    ver = container.img[\"alt\"]\n",
    "    veracity.append(ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st page df\n",
    "test_df = pd.DataFrame(\n",
    "    {'statement': statement,\n",
    "     'source': source,\n",
    "     'link': link,\n",
    "     'veracity': veracity\n",
    "})\n",
    "print(test_df.info())\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 5 pages\n",
    "pages = [str(i) for i in range(1,5)]\n",
    "start_time = time()\n",
    "requests = 0\n",
    "for _ in range(5):\n",
    "# request goes here\n",
    "    requests += 1\n",
    "    sleep(randint(1,3))\n",
    "    current_time = time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If above is problem-free, proceed to scrape all pages\n",
    "# Scraping 834 pages takes about 4-5 hours\n",
    "\n",
    "# Lists to store all the scraped data in\n",
    "statement = []\n",
    "source = []\n",
    "link = []\n",
    "veracity = []\n",
    "\n",
    "# Preparing the monitoring of the loop\n",
    "start_time = time()\n",
    "requests = 0\n",
    "\n",
    "# For every page in the interval\n",
    "pages = [str(i) for i in range(1,834)]\n",
    "for page in pages:\n",
    "\n",
    "    # Make a get request\n",
    "    response = get('https://www.politifact.com/truth-o-meter/statements/?page=' + page)\n",
    "\n",
    "    # Pause the loop in random intervals so your IP address doesn't get banned\n",
    "    sleep(randint(8,15))\n",
    "\n",
    "    # Monitor the requests\n",
    "    requests += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Select all the containers from a single page\n",
    "    statement_containers = page_html.find_all('div', class_ = 'scoretable__item')\n",
    "\n",
    "    for container in statement_containers:\n",
    "    # statement\n",
    "        sta = container.find('p', class_ = 'statement__text').get_text(strip=True)\n",
    "        statement.append(sta)\n",
    "    # source\n",
    "        sou = container.find('div', class_ = 'statement__source').get_text(strip=True)\n",
    "        source.append(sou)\n",
    "    # link\n",
    "        lin = container.find('p', class_ = 'statement__text').a[\"href\"]\n",
    "        link.append(lin)\n",
    "    # veracity\n",
    "        ver = container.img[\"alt\"]\n",
    "        veracity.append(ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe\n",
    "politifact_df = pd.DataFrame(\n",
    "    {'statement': statement,\n",
    "     'source': source,\n",
    "     'link': link,\n",
    "     'veracity': veracity\n",
    "})\n",
    "print(politifact_df.info())\n",
    "politifact_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "politifact_df.to_csv('politifact.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial followed to create this scraper: \n",
    "# https://www.dataquest.io/blog/web-scraping-beautifulsoup/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
